
# configuration groups
defaults:
  - encoder: BERT
  - train: biencoder_local
  - datasets: encoder_train_default
  - defense: fp

name: dpr

hydra:
  # sets output paths for all file logs to `logs/experiment/name'
  run:
    dir: outputs/${name}/train

rag_victim_llm_model_name: bert
rag_teacher_llm_model_name: qwen2.5-7b
action: train
pretrained_model_cfg: bert
poison_rate: 0.1
lowup_train_batch_size: 32
lowup_sample_capacity: 32
weight_decay: 0.01
attack_method: badnets
dataset_dir: 
analyze_title: 'none'
analyze_sample_tot: 1000
output_hidden_states: 0
dataset_name: nq
checkpoint_index: -1
momentum_action: record
safeloss_usemo: 0
margin: 0.1
analyze_feature: 0
train_mode: loss
sampling_method: random
distance_metric: cosine
loss_function: triplet
sactter_per_samples: 1000
attack_with_semantics: 1
load_saved_model: 0

epochs: 10
batch_size: 64
train_capacity:
train_datasets:
dev_datasets:
dev_capacity:
output_dir:
train_sampling_rates:
loss_scale_factors:

# Whether to lower case the input text. Set True for uncased models, False for the cased ones.
do_lower_case: True

val_av_rank_start_epoch: 0
seed: 12345
checkpoint_file_name: dpr_biencoder

# A trained bi-encoder checkpoint file to initialize the model
model_file:

# TODO: move to a conf group
# local_rank for distributed training on gpus

# TODO: rename to distributed_rank
local_rank: -1
global_loss_buf_sz: 592000
device: cuda:0
distributed_world_size:
distributed_port:
distributed_init_method:

no_cuda: False
n_gpu: 1
fp16: False

# For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
#        "See details at https://nvidia.github.io/apex/amp.html
fp16_opt_level: O1

# tokens which won't be slit by tokenizer
special_tokens:

ignore_checkpoint_offset: False
ignore_checkpoint_optimizer: False
ignore_checkpoint_lr: False

# set to >1 to enable multiple query encoders
multi_q_encoder: False

# Set to True to reduce memory footprint and loose a bit the full train data randomization if you train in DDP mode
local_shards_dataloader: False

exclude_ptb: False
neg_only: False