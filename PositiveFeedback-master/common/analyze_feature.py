import os
import sys
import json
import torch
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from collections import defaultdict, Counter
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN, KMeans
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix
from scipy.spatial.distance import pdist, squareform
from icecream import ic as printic


def visualize(data, labels, output_dir, title=None, alpha=1):
    if isinstance(data, torch.Tensor):
        data_np = data.cpu().numpy()
    if isinstance(labels, torch.Tensor):
        labels = labels.cpu().numpy()

    tsne = TSNE(n_components=2, random_state=42)
    reduced_data = tsne.fit_transform(data_np)
    reduced_data_list = reduced_data.tolist()

    if isinstance(labels, list):
        labels_list = labels
    else:
        labels_list = labels.tolist()
    # ä¿å­˜ä¸º JSON æ–‡ä»¶
    with open(os.path.join(output_dir, "reduced_feature_datas.json"), "w") as f:
        json.dump(reduced_data_list, f)

    with open(os.path.join(output_dir, "reduced_feature_labels.json"), "w") as f:
        json.dump(labels_list, f)

    plt.figure(figsize=(8, 8))
    unique_labels = np.unique(labels)
    for label in unique_labels:
        label_mask = labels == label
        if label == 0:
            label_str = "clean"
        else:
            label_str = "poisoned"
        plt.scatter(
            reduced_data[label_mask, 0],
            reduced_data[label_mask, 1],
            label=label_str,
            alpha=alpha,
        )

    plt.title(title)
    plt.legend()
    plt.savefig(
        os.path.join(output_dir, f"{title}.png"),
        dpi=300,
        bbox_inches="tight",
        transparent=True,
    )
    plt.close()


import os
import json
import numpy as np
from collections import Counter
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc
import matplotlib.pyplot as plt
from scipy.spatial.distance import pdist, squareform


class TempDefenseManager:
    def __init__(self, output_dir):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.best_results = defaultdict(lambda: {"f1": -1.0, "metrics": None})

    def _preprocess_data(self, rep, n_components=-1):
        """æ•°æ®é¢„å¤„ç†ï¼ˆä¸­å¿ƒåŒ– + PCAé™ç»´ï¼‰"""
        X = rep - np.mean(rep, axis=0)
        if n_components > 0:
            decomp = PCA(n_components=n_components, whiten=True)
            decomp.fit(X)
            X = decomp.transform(X)
        return X

    def _save_results(self, results, filename):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        output_path = os.path.join(self.output_dir, filename)
        with open(output_path, "w") as f:
            json.dump(results, f, indent=4)
        # print(f"ğŸ“„ Results saved to {output_path}")
        printic(output_path)

    def _evaluate_clusters(self, cluster_labels, true_labels):
        """è¯„ä¼°æ‰€æœ‰ç°‡ä½œä¸ºæ­£ç±»çš„æŒ‡æ ‡ï¼Œè¿”å›æœ€ä½³ç»“æœ"""
        best_metrics = {"f1": -1.0}
        unique_clusters = np.unique(cluster_labels)

        # éå†æ¯ä¸ªç°‡ä½œä¸ºæ­£ç±»
        for cluster in unique_clusters:
            # ç”Ÿæˆé¢„æµ‹æ ‡ç­¾ï¼ˆå½“å‰ç°‡ä¸º1ï¼Œå…¶ä»–ä¸º0ï¼‰
            preds = np.where(cluster_labels == cluster, 1, 0)

            # è®¡ç®—æ··æ·†çŸ©é˜µ
            try:
                tn, fp, fn, tp = confusion_matrix(true_labels, preds).ravel()
            except ValueError:
                continue  # è·³è¿‡å…¨0æˆ–å…¨1çš„æƒ…å†µ

            # è®¡ç®—æŒ‡æ ‡
            try:
                precision = precision_score(true_labels, preds, zero_division=0)
                recall = recall_score(true_labels, preds, zero_division=0)
                f1 = f1_score(true_labels, preds, zero_division=0)
                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
            except:
                continue

            # æ›´æ–°æœ€ä½³æŒ‡æ ‡
            if f1 > best_metrics["f1"]:
                best_metrics = {
                    "f1": float(f1),
                    "precision": float(precision),
                    "recall": float(recall),
                    "fpr": float(fpr),
                    "confusion_matrix": {
                        "tn": int(tn),
                        "fp": int(fp),
                        "fn": int(fn),
                        "tp": int(tp),
                    },
                }

        return best_metrics

    def defense_by_ac(self, rep, labels, n_components=-1):
        """åŸºäºKMeansçš„å¼‚å¸¸èšç±»æ£€æµ‹ï¼ˆéå†æ‰€æœ‰ç°‡é€‰æ‹©æœ€ä½³F1ï¼‰"""
        # æ•°æ®é¢„å¤„ç†
        X = self._preprocess_data(rep.cpu().numpy(), n_components)
        # print(f"ğŸ”§ Processed data shape: {X.shape}")
        printic(X.shape)

        # KMeansèšç±»
        kmeans = KMeans(n_clusters=2, random_state=233).fit(X)
        cluster_labels = kmeans.labels_
        # print(f"ğŸ“Š Cluster distribution: {Counter(cluster_labels)}")
        printic(Counter(cluster_labels))

        # è¯„ä¼°æ‰€æœ‰ç°‡å¹¶é€‰æ‹©æœ€ä½³æŒ‡æ ‡
        metrics = self._evaluate_clusters(cluster_labels, labels)
        metrics["method"] = "AC"

        # æ‰“å°å’Œä¿å­˜ç»“æœ
        # print(f"[AC] Best metrics = {json.dumps(metrics, indent=4)}")
        printic(metrics)
        self._save_results(metrics, "defenseByAC.json")
        return metrics

    def defense_by_dbscan(self, rep, labels, eps=10, min_samples=30, n_components=-1):
        """åŸºäºDBSCANçš„å¯†åº¦å¼‚å¸¸æ£€æµ‹ï¼ˆéå†æ‰€æœ‰ç°‡é€‰æ‹©æœ€ä½³F1ï¼‰"""
        # æ•°æ®é¢„å¤„ç†
        X = self._preprocess_data(rep.cpu().numpy(), n_components)

        # DBSCANèšç±»
        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric="euclidean")
        cluster_labels = dbscan.fit_predict(X)
        # print(f"ğŸ” Unique clusters: {np.unique(cluster_labels)}")
        printic(np.unique(cluster_labels))

        # è¿‡æ»¤å™ªå£°ç‚¹ï¼ˆ-1æ ‡ç­¾ä¸å‚ä¸è¯„ä¼°ï¼‰
        valid_mask = cluster_labels != -1
        filtered_clusters = cluster_labels[valid_mask]
        print(f"filtered_clusters = {filtered_clusters}")
        filtered_labels = labels[valid_mask]

        # è¯„ä¼°æœ‰æ•ˆç°‡å¹¶é€‰æ‹©æœ€ä½³æŒ‡æ ‡
        metrics = self._evaluate_clusters(filtered_clusters, filtered_labels)
        metrics["method"] = "DBSCAN"

        # æ‰“å°å’Œä¿å­˜ç»“æœ
        printic(metrics)
        self._save_results(metrics, "defenseByDBSCAN.json")
        return metrics

    def save_best_results(self):
        """ä¿å­˜æ‰€æœ‰æ–¹æ³•çš„æœ€ä½³ç»“æœåˆ°æ–‡ä»¶"""
        best_output = {
            method: data["metrics"]
            for method, data in self.best_results.items()
            if data["metrics"] is not None
        }
        best_path = os.path.join(self.output_dir, "best_results.json")
        with open(best_path, "w") as f:
            json.dump(best_output, f, indent=4)
        # print(f"ğŸš€ Best results saved to {best_path}")
        printic(best_path)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–é˜²å¾¡ç®¡ç†å™¨
    defender = TempDefenseManager(output_dir="./defense_results")

    # æ¨¡æ‹Ÿæ•°æ®
    rep = torch.randn(100, 256)  # å‡è®¾çš„è¡¨ç¤ºæ•°æ®
    labels = np.random.randint(0, 2, 100)  # æ¨¡æ‹Ÿæ ‡ç­¾

    # æ‰§è¡ŒACæ£€æµ‹
    ac_metrics = defender.defense_by_ac(rep, labels)

    # æ‰§è¡ŒDBSCANæ£€æµ‹
    dbscan_metrics = defender.defense_by_dbscan(rep, labels)

    # æ‰§è¡Œç›¸ä¼¼åº¦åˆ†æï¼ˆéœ€å®é™…æ•°æ®ï¼‰
    # defender.defense_by_similarity(distance_func, rep_anchor, rep_pos, poison_labels)
